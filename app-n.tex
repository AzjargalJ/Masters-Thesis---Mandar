\chapter{Subroutines/Pseudo codes}
\label{app:n}
This appendix contains the pseudo code of major components of the FS-Scala implementation. They are organized as per their function in the model building process.

\section{FS-LSSVM}

\begin{algorithm}[!htbp] \label{lssvmalgo}
\SetAlgoLined
\KwData{Data Set, Kernel, Global Optimization routine, grid parameters}
\KwResult{Proposed Tuned FS-LSSVM model}
 Pre-process the data by mean scaling.\;
 \textbf{Calculate the prototype set by maximizing the Quadratic R\`enyi Entropy in parallel using MapReduce.}\;
 Initialize a grid for the hyper-parameters\;
 \While{termination of global optimization routine}{
  Initialize the kernel using the hyper-parameters.
  Do AFE on the kernel matrix constructed from the prototypes, using the Nystrom method\;
  evaluate the cross validation score for the particular hyper-parameter values\;
 }
 \caption{Tuning FS-LSSVM}
\end{algorithm}


\section{Model Training}

\begin{algorithm}[!ht]\label{fmmr}
    \DontPrintSemicolon
    \KwData{$X = [x^i],\ x^i\ \epsilon \ \mathbf{R}^n$, 
    $\hat{\phi} : \mathbf{R}^n \longrightarrow \mathbf{R}^m$, 
    $Y = [y^i], y^i \epsilon \mathbf{R}$}
    \KwResult{$\left ( \hat{\Phi}^{\intercal}_e \hat{\Phi}_e \right ),\ \hat{\Phi}^{\intercal}_e Y$}
    \Begin{
        $MapFn(x, y)$:\;
        \ \   $M \longleftarrow \hat{\phi}(x) \hat{\phi}(x)^T$\;
        \ \   $v \longleftarrow \hat{\phi}(x) \ y$\;
        $emit(M,v)$\;
    }
    \Begin{
        $RedFn((M, v), (M', v'))$:\;
        $emit(M + M', v + v')$\;
    }
    \Begin{
        $(F, v) \longleftarrow MapReduce(X, MapFn, RedFn)$\;
        
        return $(F, v)$\;
    }
\caption{Calculate feature matrices from data using MapReduce: $FeatureMat$}
\end{algorithm}

\begin{algorithm}[!ht]\label{cgmr}
    \DontPrintSemicolon
    \KwData{$X = [x^i],\ x^i\ \epsilon \ \mathbf{R}^n$, 
    $\hat{\phi} : \mathbf{R}^n \longrightarrow \mathbf{R}^m$, 
    $Y = [y^i], y^i \epsilon \mathbf{R}$, $\gamma$, $\epsilon$}
    \KwResult{$\left( \begin{matrix}
\hat{w}\\ 
\hat{b}
\end{matrix}\right ) = 
\left ( \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma} \right )^{-1} \hat{\Phi}^{\intercal}_e Y$}
    \Begin{
        $(F, v) \longleftarrow FeatureMat(X, Y, \hat{\phi}, \gamma)$ \;
        $A \longleftarrow F + \frac{1}{\gamma} \mathbf{I}_{m \times m}$\;
        \BlankLine
        \nl\While{not $max iterations$ and $\Delta(\hat{w}, \hat{b}) \geq \epsilon$}{
            $(\hat{w}_{i+1}, \hat{b}_{i+1}) \longleftarrow CGUpdate(\hat{w}_{i}, \hat{b}_{i}, A, v)$\;
            $\Delta(\hat{w}, \hat{b}) \longleftarrow \left \| \hat{w}_{i+1}  - \hat{w}_{i}\right \|^2 + \left \| \hat{b}_{i+1}  - \hat{b}_{i}\right \|^2$
        }
    }
\caption{Conjugate Gradient: $CG$}
\end{algorithm}

\section{Model Evaluation}

\begin{algorithm}[!ht]\label{cvmr}
    \DontPrintSemicolon
    \KwData{$X = [x^i],\ x^i\ \epsilon \ \mathbf{R}^n$, 
    $\hat{\phi} : \mathbf{R}^n \longrightarrow \mathbf{R}^m$, 
    $Y = [y^i], y^i \epsilon \mathbf{R}$, $\gamma$, folds}
    \KwResult{Cross Validation Performance}
    
    \Begin{
        $(A, v) \longleftarrow FeatureMat(X, Y, \hat{\phi}, \gamma)$ \;
        $score \longleftarrow 0$\;
        \BlankLine
        \nl\For{$i \longleftarrow 1 \ to \ folds $}{
            $(X_i, Y_i) \longleftarrow$ fold i\;
            $(A_i, v_i) \longleftarrow FeatureMat(X_i, Y_i, \hat{\phi}, \gamma)$ \;
            $(\hat{w}, \hat{b}) \longleftarrow CG(A - A_i + \frac{1}{\gamma} \mathbf{I}_{m \times m}, v - v_i)$\;
            $score \longleftarrow score + evaluateFold(\hat{w}, \hat{b}, X_i, Y_i)$\;
        }
        return $score/folds$\;
    }
\caption{Distributed v-Fold Cross-Validation}
\end{algorithm}

\begin{algorithm}[!ht]\label{efmr}
    \DontPrintSemicolon
    \KwData{$X_f = [x^i],\ x^i\ \in \ \mathbf{R}^n$, 
    $\hat{\phi} : \mathbf{R}^n \longrightarrow \mathbf{R}^m$, 
    $Y_f = [y^i], y^i \in \mathbf{R}$, $\hat{w}$, $\hat{b}$.}
    \KwResult{score for given fold}
    \Begin{
        $predictLabel(\hat{w}, \hat{b})(x, y)$:\;
        $emit(\hat{w}\cdot x + \hat{b},y)$\;
    }
    \Begin{
        $Vector.fill(length)(IndicatorFn)$:\;
        $vec \longleftarrow (0, ..., 0)_{length} \ map (IndicatorFn)$\;
        $return(vec)$\;
    }
    \Begin{
        $MapScore(score, label)$:\;
        \eIf{label = 1.0}{
            $Pos \longleftarrow Pos + 1$\;
            $tpv \longleftarrow Vector.fill(l)(IndicatorFn(sign(score - thresholds(i)) == 1.0))$\;
            $fpv \longleftarrow Vector.fill(l)(IndicatorFn(false))$\;
        }{
            $Neg \longleftarrow Neg + 1$\;
            $tpv \longleftarrow Vector.fill(l)(IndicatorFn(false))$\;
            $fpv \longleftarrow Vector.fill(l)(IndicatorFn(sign(score - thresholds(i)) == 1.0))$\;
        }
        $emit(tpv,fpv)$\;
    }
    \Begin{
        $RedScore((u, v), (u', v'))$:\;
        $emit(u + u', v + v')$\;
    }
    \Begin{
        $thresholds \longleftarrow List(t_1, t_2, \ldots t_l)$\;
        $Pos \longleftarrow 0$\;
        $Neg \longleftarrow 0$\;
        $scoresLabels \longleftarrow (X_f, Y_f)\ map\ predictLabel(\hat{w}, \hat{b})$\;
        $(tp, fp) \longleftarrow scoresLabels \ map(MapScore) \ reduce(RedScore)$\;
        \BlankLine
        $tp \longleftarrow tp / Pos$\;
        $fp \longleftarrow fp / Neg$\;
        $roc \longleftarrow thresholds\ zip(tp\ zip\ fp)$\;
        \BlankLine
        
        return $1 - area(roc)$\;
    }
\caption{Evaluate performance for fold: $evaluateFold$}
\end{algorithm}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
