\chapter{Distributed Computation Support}
\label{cha:3}

\section{Map Reduce}

Although \textit{MapReduce} \cite{MapReduce} has been the focal point of the large scale machine learning surge, as a computational paradigm it is hardly new. Programming languages with FP capabilities (ex. Haskell, Scheme, Scala) have operations \texttt{map} and \texttt{reduce} on various data immutable (and sometimes mutable) data structures like Maps, Lists, Arrays, etc. One can argue that \textit{MapReduce} which relies on the "code as data" philosophy is quite inherent in the FP paradigm.

It is worthwhile to note that although \textit{MapReduce} is supported in several programming languages, the current distributed implementations of \textit{MapReduce} like \textit{Apache Hadoop} \cite{Hadoop:2005} and \textit{Apache Spark} have gained popularity because of their ability to distribute these computations across massive clusters. We inspect \textit{Map Reduce} in more detail to understand how this capability is achieved. Figure \ref{fig:mapreduce} below shows a schematic picture of distributed \texttt{MapReduce}.

\tikzstyle{block} = [draw,fill=blue!20,minimum size=2em]
% diameter of semicircle used to indicate that two lines are not connected
\def\radius{.7mm} 
\tikzstyle{branch}=[fill,shape=circle,minimum size=3pt,inner sep=0pt]
\begin{figure}[!ht] 
\begin{adjustbox}{max width=\textwidth}
\begin{tikzpicture}[>=latex']
    \node (rect) at (0,-1) [draw,thick,fill=blue!20,minimum width=5em,minimum height=14em] {data};
    % Draw blocks, inputs and outputs
    \foreach \y in {1,2} {
        \node[block] at (10,-\y+0.5) (red\y) {$reduceTask_\y$};
        \draw[->] (red\y.east) -- +(0.5,0);
    }
    
    \foreach \y in {1,2,3,4,5} {
        \node[block] at (5,-\y+2) (map\y) {$mapTask_\y$};
        \draw[->] (rect) -- (map\y);
        \draw[->] (map\y.east) -- (red1.west);
        \draw[->] (map\y.east) -- (red2.west);
    }

\end{tikzpicture}
\end{adjustbox}
\caption{Schematic Diagram of Distributed Map Reduce}
\label{fig:mapreduce}
\end{figure}

It can be broadly divided into two major phases or steps, although there are more phases that can be optionally applied, the reader is urged to refer to references in this area for a more in-depth explanation \cite{HDFS}, \cite{HadoopBook} \cite{Yarn}.

\begin{itemize}
\item \textit{Map}: Each file is divided into splits, on each node/split a \texttt{map} function (of the form $V \longrightarrow W$) which is self contained (it is assumed to be independent of data on other splits) is applied on each data split. Due to this inter-partition \texttt{map} operation independence, this phase can be carried out in a (massively) parallel fashion.

\item \textit{Reduce}: This consists of application of the so called \texttt{reduce} function which is of the form $W \times W \longrightarrow W$, it "combines" or "merges" two quantities/entities which are outputs of the \texttt{map} function in some meaningful way. In order for this phase to be implemented in parallel, it is required that this \texttt{reduce} function be associative in the algebraic sense, although commutativity is not a requirement of this function, it allows for further distributed optimization of this computation.

\item \textit{Combine} (Optional): This is an optional phase which may be applied before the \textit{Reduce} phase, it is a function called \texttt{combine} whose functional signature is identical to that of the \texttt{reduce} function, its utility is primarily reducing the overhead on the \textit{Reduce} phase by preprocessing the intermediate output of the \texttt{map} functions. 
\end{itemize}


\section{Application in FS-Scala}
FS-Scala leverages \textit{MapReduce} in two separate stages as outlined below, using \textit{Apache Spark} to distribute the workload amongst a number of processes/threads. Appendix \ref{app:n} contains the detailed pseudo code for all the computations outlined below
\begin{itemize}

\item Model Training:
For training a single FS-LSSVM model instance (given a fixed value of hyper-parameters), one must apply the feature map $\phi$ on each training example and generate the extended feature matrix $\hat{\Phi}_e$ in order to solve the linear system in equation \eqref{eqfssol} chapter \ref{cha:1}, this is achieved by algorithm \ref{fmmr}. Algorithm \ref{cgmr} outlines the procedure to estimate the parameters $\hat{w},\ \hat{b}$ of the FS-LSSVM model discussed in section \ref{sec:fs}, using the \textit{Conjugate Gradient} algorithm. Due to the primal formulation of the FS-LSSVM, the size of matrix $A =  \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma}$ , in the linear system in \eqref{eqfssol} is $(m+1) \times (m+1)$, where $m$ is the number of prototypes selected to construct the kernel matrix in kernel based FS-LSSVM.

\item Model Evaluation
In the model evaluation process, there are two tasks which need to be solved with respect to calculating the cost of performance of a particular model instance.
\begin{itemize}
\item v-fold Cross Validation:
The fast v-fold cross-validation outlined by De Brabanter et.al. \cite{DeBrabanter2010} is implemented in algorithm \ref{cvmr} to reduce the execution time for calculating the feature matrices. 
\item Performance Metrics:
Algorithm \ref{efmr} contains the implementation for calculating test set performance for each fold of test data. The true positive versus false positive curve is calculated on the test/validation fold provided and used to calculate the ROC area.
\end{itemize}
\end{itemize}
