\chapter{Distributed Computation Support}
\label{cha:3}

\section{Map Reduce}

Although \textit{MapReduce} \cite{MapReduce} has been the focal point of the large scale machine learning surge, as a computational paradigm it is hardly new. Programming languages with FP capabilities (ex. Haskell, Scheme, Scala) have operations \texttt{map} and \texttt{reduce} on various data immutable (and sometimes mutable) data structures like Maps, Lists, Arrays, etc. One can argue that \textit{MapReduce} which relies on the "code as data" philosophy is quite inherent in the FP paradigm.

It is worthwhile to note that despite the novelty in the current distributed implementations of \textit{MapReduce} like \textit{Apache Hadoop} \cite{Hadoop:2005} and \textit{Apache Spark} have gained popularity because of their ability to distribute these computations across massive clusters. We inspect \textit{Map Reduce} in more detail to understand how this capability is achieved. Figure \ref{fig:mapreduce} below shows a schematic picture of distributed \texttt{MapReduce}.

\tikzstyle{block} = [draw,fill=blue!20,minimum size=2em]
% diameter of semicircle used to indicate that two lines are not connected
\def\radius{.7mm} 
\tikzstyle{branch}=[fill,shape=circle,minimum size=3pt,inner sep=0pt]
\begin{figure}[!ht] 
\begin{adjustbox}{max width=\textwidth}
\begin{tikzpicture}[>=latex']
    \node (rect) at (0,-1) [draw,thick,fill=blue!20,minimum width=5em,minimum height=14em] {data};
    % Draw blocks, inputs and outputs
    \foreach \y in {1,2} {
        \node[block] at (10,-\y+0.5) (red\y) {$reduceTask_\y$};
        \draw[->] (red\y.east) -- +(0.5,0);
    }
    
    \foreach \y in {1,2,3,4,5} {
        \node[block] at (5,-\y+2) (map\y) {$mapTask_\y$};
        \draw[->] (rect) -- (map\y);
        \draw[->] (map\y.east) -- (red1.west);
        \draw[->] (map\y.east) -- (red2.west);
    }

\end{tikzpicture}
\end{adjustbox}
\caption{Schematic Diagram of Distributed Map Reduce}
\label{fig:mapreduce}
\end{figure}

It can be broadly divided into two major phases or steps, although there are more phases that can be optionally applied, the reader is urged to refer to references in this area for a more in-depth explanation \cite{HDFS}, \cite{HadoopBook} \cite{Yarn}.

\begin{itemize}
\item \textit{Map}: Each file is divided into splits, on each node/split a \texttt{map} function (of the form $V \longrightarrow W$) which is self contained (it is assumed to be independent of data on other splits) is applied on each data split. Due to this inter-partition \texttt{map} operation independence, this phase can be carried out in a (massively) parallel fashion.

\item \textit{Reduce}: This consists of application of the so called \texttt{reduce} function which is of the form $W \times W \longrightarrow W$, it "combines" or "merges" two quantities/entities which are outputs of the \texttt{map} function in some meaningful way. In order for this phase to be implemented in parallel, it is required that this \texttt{reduce} function be associative in the algebraic sense, although commutativity is not a requirement of this function, it allows for further distributed optimization of this computation.

\item \textit{Combine} (Optional): This is an optional phase which may be applied before the \textit{Reduce} phase, it is a function called \texttt{combine} whose functional signature is identical to that of the \texttt{reduce} function, its utility is primarily reducing the overhead on the \textit{Reduce} phase by preprocessing the intermediate output of the \texttt{map} functions. 
\end{itemize}


\section{Application in FS-Scala}

As discussed above, we use \textit{MapReduce} wherever possible in order to distribute the workload using \textit{Apache Spark}. Due to the primal formulation of the FS-LSSVM, the size of matrix $A =  \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma}$ , in the linear system in \eqref{eqfssol} is $(m+1) \times (m+1)$, where $m$ is the number of prototypes selected to construct the kernel matrix in kernel based FS-LSSVM. Procedure \ref{cgmr} outlines the procedure to estimate the parameters $\hat{w},\ \hat{b}$ of the FS-LSSVM model discussed in section \ref{sec:fs}, using the \textit{Conjugate Gradient} algorithm. 

Using \textit{MapReduce}, we calculate $A = \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma}$ and $\hat{\Phi}^{\intercal}_e Y$. We use these results to carry out iterations of the Conjugate Gradient updates until the maximum number of iterations is reached.

Appendix \ref{app:n} contains the detailed pseudo code for all the procedures discussed above. 