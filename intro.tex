\chapter{Introduction}
\label{cha:intro}
The $21^{st}$ century stands out in how mankind learned the value of storing and making predictions/decisions from large volumes of data. A significant aspect of large scale data analysis is distributed computation frameworks like \textit{High Performance Computing}, \textit{Message Passing Interface} etc. Recently large scale commodity hardware clusters have replaced the two former frameworks as the most popular model for parallel data analysis. With this crucial change in hardware came a change in computational models as well. It is at this juncture that distributed \textit{Map Reduce} became the de-facto computational philosophy for large scale data analysis and  words such as \textit{Hadoop} \cite{Hadoop:2005, chang2008bigtable, Borthakur2011} and \textit{Apache Spark} \cite{Zaharia2010, Spark:2010} have become synonymous with large scale data analysis and machine learning.

Along with innovation in hardware design and distributed computing models, there came a need for good programming libraries and frameworks to work with various Machine Learning models on large data sets. It was demonstrated in \cite{10.1109/MIS.2009.36} that a gigantic language corpus encapsulates almost all aspects of human language and speech. So far the prevalent `motto' in the Internet industry has been ``large data, simple models". Often, this is misunderstood as the Machine Learning translation of \textit{Occam's Razor}. The bias-variance trade-off \cite{Valentini2004} (appendix \ref{app:biasvar}) is a far better mechanism to ensure that a model does not become overly complex, and this, rather than restricting the user to simple models, is the real Occam's razor in training a model. 

Therefore, in order to extract maximum value from large scale data, it is important to have the flexibility to train and compare different model families before arriving at the one that fits the requirement of the user. Therefore one must be able to train general nonlinear models and tweak them by changing the various components which they employ to learn (i.e., a model may be linear or kernel based, it can be optimized by various methods like \textit{Stochastic Gradient Descent}, \textit{Conjugate Gradient}, etc.). This is not possible in a rigid, monolithic programming framework. Modularity, extensibility and ease of usage are of paramount importance while designing Machine Learning software for large scale data applications.

Scala \cite{scala-overview-tech-report}, a multi-paradigm Java Virtual Machine (JVM) based programming language has gained popularity for its expressiveness and performance. It's power and easy interpolability with Java has contributed to its adoption in software architectures that heavily rely on Java and its ecosystem. The current state of the art in distributed Machine Learning in Scala is the \textit{MLLib} module in \textit{Apache Spark} \cite{Meng}. It has implementations of Linear SVM and Logistic Regression for solving binary classification problems. But a crucial component missing in \textit{MLLib} and all distributed Machine Learning libraries is the ability to learn classification models with nonlinear decision boundaries. FS-Scala aims to solve the problem of scalable non-linear classification models by implementing the \textit{Fixed-Size Least Squares Support Vector Machine} (FS-LSSVM) algorithm \cite{DeBrabanter2010,Suykens2002} with model tuning capabilities.

In recent literature we find sparse reductions to FS-LSSVM methods \cite{Mall2015,Mall2013}. The authors in \cite{Mall2015,Mall2013} explored the sparsity vs error trade-off \footnote{Sparsity vs error trade off refers to how the model error increases when the number of prototype vectors is reduced via the $L_0$ regularization on model parameters.} for FS-LSSVM models. Even though they run experiments on large scale data sets like \emph{Forest Cover}, the scalability of these methods are restricted to available memory on a single machine. Moreover, they don't exploit the possibility of parallelism available in several components of the FS-LSSVM model. 

Another work \cite{Mall2014} converts the Big Data into a Big Network and then uses a network based subset selection technique (\textit{Fast and Unique Representative Subset selection} (FURS) \cite{Mall2013FURS}) to obtain a representative subset of the original data. It then builds a FS-LSSVM model using this subset. However, in this thesis we showcase that we can distribute the subset selection computation which maximizes the \textit{Quadratic R\`enyi Entropy} for Big data sets and use the generated subset as the set of prototype vectors (PV) essential for building the FS-LSSVM model.

The rest of this thesis is organized as follows.

\begin{itemize}
\item Chapter \ref{cha:1} briefly discusses the classical Support Vector Machine (SVM) as well as the LSSVM formulations. In the case of the LSSVM, the dual formulation is discussed motivating the introduction of the FS-LSSVM which solves the LSSVM in the primal for large data sets.

\item Chapter \ref{cha:2} compares and contrasts various programming languages and paradigms, justifying the choice of Scala in the implementation of the FS-LSSVM. The rest of the chapter focuses on the salient features of the current SVM and LSSVM software implementations and motivates the issues tackled by FS-Scala. Chapter \ref{cha:2} concludes with the contributions of \emph{FS-Scala} in LSSVM software design and implementation.

\item Chapter \ref{cha:3} introduces distributed \emph{MapReduce} and details the computational phases of the FS-LSSVM which are performed in a distributed manner in \emph{FS-Scala}.

\item Chapter \ref{cha:n} discusses experiments conducted using \emph{FS-Scala} on the \emph{Magic Gamma}, \emph{Adult} and \emph{Forest Cover Type} data and tabulates the performance of various kernel based as well as linear FS-LSSVM models on the selected data sets.

\item The thesis concludes \ref{cha:conclusion} with a discussion of the results of experiments of chapter \ref{cha:n} and further research directions that can be pursued with respect to the development of \emph{FS-Scala}.
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
