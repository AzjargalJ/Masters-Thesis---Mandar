\chapter{Conclusions and Future Work}\label{cha:conclusion}

In this thesis, \emph{FS-Scala} a Scala-based implementation of the FS-LSSVM is proposed as a programming framework for working with small and large scale FS-LSSVM models. The current SVM and LSSVM software do not leverage distributed \emph{MapReduce} to implement kernel based SVM/LSSVM models but rather provide support for only linear models in the case of large data sets (\emph{Apache Spark} \cite{Spark:2010}). 

The proposed library  \emph{FS-Scala} takes advantage of the latest developments in distributed data processing as well as functional and object oriented programming to create a robust and performance oriented LSSVM research and development framework that can be used to train advanced LSSVM models on large and small data sets. Emphasis on modularity of design helps \emph{FS-Scala} towards becoming an attractive option for cutting edge Machine Learning research in the area of kernel based models.

\emph{FS-Scala} includes global optimization algorithms like \emph{Grid Search} (GS) and \emph{Coupled Simulated Annealing} (CSA) used to tune FS-LSSVM models. In order to make the tuning of non linear classification models possible on large data sets, a number of performance enhancements are proposed in section \ref{contributions}, which include but not limited to fast v-fold cross validation and feature matrix caching.

In chapter \ref{cha:n}, \emph{FS-Scala} is applied on a number of benchmark data sets (i.e. \emph{Magic Gamma}, \emph{Adult} and \emph{Forest Cover Type}) and the performance and model tuning time are recorded. It is observed that our implementation enables scalable training, tuning and evaluation of LSSVM models, while still providing flexibility to tweak various underlying data processing infrastructure. We conclude by discussing ideas for future research work and further improvements to \emph{FS-Scala}. 

\section*{Future Work}

Further research/improvements on \emph{FS-Scala} can be done in two proposed directions.

\begin{itemize}
\item Experiments of large scale commodity clusters:
Simulations \ref{cha:n} were carried on a \emph{Apache Spark} standalone cluster which runs tasks concurrently over the different cores of the parent machine. But it is also possible to scale this process even further for greater computational benefits by using large commodity clusters as provided by cloud computing providers.

\item Simultaneous evaluation of multiple model instances:
During a particular v-fold cross validation operation only a single model instance (configuration of hyper-parameters) is trained and  evaluated, therefore to evaluate a $k \times k$ grid, $k^2$ v-fold cross validations must be carried out. Substantial improvements can be achieved if we can train and do v-fold CV on multiple model instances using only a single pass through the training data, leading to a new and exciting area of research in large scale FS-LSSVM model tuning.

\item Distributed FS-LSSVM committee models:
An exciting area of further research is application of distributed programming methodology to the implementation of FS-LSSVM committee models. \emph{Apache Spark} inherently represents large data sets as sets distributed partitions, this can be used to train a FS-LSSVM committee model where each partition trains a local LSSVM model using only its local data. The models trained on each partition can now be used to train a weighted FS-LSSVM committee network.

\end{itemize}