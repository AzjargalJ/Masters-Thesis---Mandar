\chapter{Least Squares Support Vector Machines}
\label{cha:1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Support Vector Machines} \label{SVM}
 
The classical soft margin Support Vector Machine (SVM) model as proposed by Cortes et. al \cite{Cortes} relies on maximizing the margin between the separating hyper-plane while minimizing the mis-classification error on the input patterns. It is formulated in equation \eqref{eqsvm} below.

\begin{equation}\label{eqsvm}
\begin{aligned}
& \underset{w,b,e}{\text{min}} &
 \mathcal{J}(w,e) = \frac{1}{2}w^{\intercal}w + \gamma\sum\limits_{i=1}^N e_{i} \\
& \text{s.t.} &
y_{i}[ w^{\intercal}\phi(x_{i})+b ] \geq 1 - e_{i},\ i=1,\ldots ,N. \\
& & e_{i} \geq 0,\ i=1,\ldots ,N.
\end{aligned}
\end{equation}

The functions $\phi(x)$ are suitably chosen basis functions, which describe an inner product space called a \textit{Reproducing Kernel Hilbert Space} (RKHS) \cite{N.Aronszjan1950}. The RKHS generated by $\phi(x)$ can be identified by a positive semi-definite Kernel function $K(x, y) = \phi(x)^{T}\phi(y)$, a class of kernel functions popularly known as \textit{Mercer} kernels \cite{Mercer}. Generally the product $\phi(x)^{T}\phi(y)$ is replaced by the Kernel function $K(x, y)$ without explicitly calculating or solving for $\phi(x)$, this is known in SVM literature as the \textit{kernel trick}.

It is well known that solving the optimization problem in equation \eqref{eqsvm} leads to a \textit{sparse} representation of the best separating hyper-plane between the two classes in the data. This can be achieved by introducing Lagrange multipliers and applying the Karush Kuhn Tucker (KKT) conditions yielding a classifier of the form $w = \sum\limits_{k=1}^N \alpha_k y_k \phi(x_{i})$. Most of the multipliers $\alpha_k$ are zero due to the KKT complementary slackness condition, while the non zero multipliers determine the \textit{support vectors}. How ever the classical soft margin SVM formulation also leads to a \textit{Quadratic Programming} (QP) problem in terms of the Lagrange multipliers which often leads to $O(N^3)$ time complexity in the model training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least Squares Support Vector Machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Least Squares Support Vector Machines (LSSVM) proposed by Suykens et.al \cite{Suykens2002, Suykens1999} modify the classical soft margin SVM formulation above by replacing the \textit{hinge} loss function in \eqref{eqsvm} with the \textit{squared error} loss function and the inequality constraints (in terms of the error/slack variables $e_i$) with equality constraints, we obtain the following optimization problem \eqref{eqlssvm}.
\begin{equation}\label{eqlssvm}
\begin{aligned}
& \underset{w,b,e}{\text{min}}
& & \mathcal{J}(w,e) = \frac{1}{2}w^{\intercal}w + \frac{\gamma}{2}\sum\limits_{i=1}^N e_{i}^2 \\
& \text{s.t.}
& & y_{i}[ w^{\intercal}\phi(x_{i})+b ] = 1 - e_{i}, i=1,\ldots ,N.
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Introducing the Lagrangian and applying the KKT conditions gives us the solution of the problem in the dual \eqref{eqkkt}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eqkkt}
\left[\begin{array}{c|c}
   0  & y^\intercal   \\ \hline
   y & \Omega + \gamma^{-1} \mathit{I} 
\end{array}\right] 
\left[\begin{array}{c}
   b    \\ \hline
   \alpha  
\end{array}\right] = \left[\begin{array}{c}
   0    \\ \hline
   1_v  
\end{array}\right],
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
In the above equation, the quantities $\Omega_{kl}$, $\alpha$ and $K(x_k,x_l)$ are given by the following expressions.
\begin{equation*}
\begin{aligned}
& \Omega_{kl} = y_{k}y_{l}K(x_{k}, x_{l}) \\
& \alpha = \left[\alpha_1 ; ... ; \alpha_N \right] \\
& K(x_k,x_l) = \phi(x_k)^\intercal\phi(x_l) \\
\end{aligned}
\end{equation*}

This solution implies a loss of sparsity as compared to the classical SVM since each point becomes a support vector. However, we gain linearity of the solution (i.e. we do not have to solve the \textit{Quadratic Programming} problem as in the classical SVM). This affords us greater freedom in choosing optimization algorithms to solve the formulation in \eqref{eqlssvm}, apart from the standard \textit{Stochastic Gradient Descent} we can also employ algorithms for linear systems such as \textit{Conjugate Gradient}, \textit{Gauss Seidel}, \textit{Jacobi} etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FS-LSSVM: Tackling Large Scale Problems} \label{sec:fs}
It is observed that solving the problem \eqref{eqkkt} in the dual is not advantageous for large scale analysis as the size of the solution matrix is equal to the size of the original data. In order to make the training of kernel based SVM models for large scale data applications feasible, one must solve the optimization problem in the primal and make approximations to the computation of the kernel matrices. The Fixed-Size LSSVM (FS-LSSVM) as proposed by De Brabanter, Suykens et. al \cite{DeBrabanter2010,Suykens2002} consists of solving the LSSVM problem in the primal as follows. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{equation}
\label{eqfs}
\min_{w,b} \ \frac{1}{2}w^{\intercal} w + \frac{\gamma}{2}\sum^{n}_{i=1} \left(y_{i} - w^{\intercal} \hat{\phi}(x_i) - b\right)^{2}.
\end{equation}

The solution to equation \ref{eqfs} is given by:
\begin{align}
\label{eqfssol}
& \left( \begin{matrix}
\hat{w}\\ 
\hat{b}
\end{matrix}\right ) = 
\left ( \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma} \right )^{-1} \hat{\Phi}^{\intercal}_e y,
\\ \nonumber \\
\text{where} \hspace{10pt}
& \hat{\Phi}_e = \begin{pmatrix}
\hat{\phi}_{1}(x_1) & \cdots & \hat{\phi}_{m}(x_1) & 1\\ 
\vdots &  \ddots & \vdots & \vdots\\ 
\hat{\phi}_{1}(x_n) & \cdots & \hat{\phi}_{m}(x_n) & 1
\end{pmatrix}. \nonumber
\end{align}

In the above formulation, $\hat{\phi}(x_k)$ is an approximation to the true feature map $\phi(x_k)$ which is related to the kernel $K(x_i, x_j) = \phi(x_i)^{\intercal} \phi(x_j)$ (Mercer's theorem). The approximate feature map $\hat{\phi}(x_k)$ is calculated using the Nystr\"om method as outlined in \cite{DeBrabanter2010, Mall2015, Mall2013}. A low rank approximation to the kernel matrix is constructed by iteratively calculating a subset of the original data which maximizes the \textit{Quadratic R\`enyi Entropy}. This procedure of extracting $\hat{\phi}(x_k)$ from a data set, given a kernel function, is called \textit{Automatic Feature Extraction} (AFE) (see appendix \ref{app:nystrom}).

Kernel based models are sensitive to hyper-parameters. In the case of FS-LSSVM we have to tune the model with respect to $\gamma$ the regularization parameter and the parameters of the kernel chosen. Models are generally compared with their cross-validation performance in which case the objective cost function with respect to the hyper-parameters is in general non-smooth and non-convex. Gradient free methods like Grid Search, Nelder Mead \cite{Nelder1965} and Coupled Simulated Annealing \cite{Xavier-De-Souza2010} are suitable to tackle the problem of model selection for FS-LSSVM based kernel models. Algorithm \ref{lssvmalgo} in appendix \ref{app:n} explains the steps involved in tuning the FS-LSSVM model with the bold part representing our contributions in this thesis, which have been implemented in a \textit{MapReduce} setting.
