\chapter{Least Squares Support Vector Machines}
\label{cha:1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least Squares Support Vector Machines} \label{LSSVM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
Least Squares Support Vector Machines (LSSVM) \cite{Suykens2002} \cite{Suykens1999} modifies the SVM formulation to include the \textit{squared error} loss function and equality constraints with respect to the error variables $e_i$, as shown in \eqref{eqlssvm}. 
\begin{equation}\label{eqlssvm}
\begin{aligned}
& \underset{w,b,e}{\text{min}}
& & \mathcal{J}(w,e) = \frac{1}{2}w^{\intercal}w + \frac{\gamma}{2}\sum\limits_{i=1}^N e_{i}^2 \\
& \text{s.t.}
& & y_{i}[ w^{\intercal}\phi(x_{i})+b ] = 1 - e_{i}, i=1,\ldots ,N.
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Introducing the Lagrangian and applying the KKT conditions gives us the solution of the problem in the dual \eqref{eqkkt}. This solution implies a loss of sparsity as compared to the classical SVM since each point becomes a support vector. However, we gain linearity of the solution (i.e. we do not have to solve the \textit{Quadratic Programming} problem as in the classical SVM). Solving the problem in the dual is not advantageous for large scale analysis as the size of the solution matrix is equal to the size of the original data.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eqkkt}
\left[\begin{array}{c|c}
   0  & y^\intercal   \\ \hline
   y & \Omega + \gamma^{-1} \mathit{I} 
\end{array}\right] 
\left[\begin{array}{c}
   b    \\ \hline
   \alpha  
\end{array}\right] = \left[\begin{array}{c}
   0    \\ \hline
   1_v  
\end{array}\right],
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
where $\Omega_{kl} = y_{k}y_{l}K(x_{k}, x_{l})$, $\alpha = \left[\alpha_1 ; ... ; \alpha_N \right]$ and $K(x_k,x_l) = \phi(x_k)^\intercal\phi(x_l)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{FS-LSSVM} \label{sec:fs}
In order to make the training of kernel based SVM models for large scale data applications feasible, one needs to make approximations to the computation of the kernel matrices. The Fixed-Size LSSVM (FS-LSSVM) as proposed by De Brabanter, Suykens et. al \cite{DeBrabanter2010,Suykens2002} consists of solving the LSSVM problem in the primal as follows.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eqfs}
\min_{w,b} \ \frac{1}{2}w^{\intercal} w + \frac{\gamma}{2}\sum^{n}_{i=1} \left(y_{i} - w^{\intercal} \hat{\phi}(x_i) - b\right)^{2}.
\end{equation}

The solution to equation \ref{eqfs} is given by:
\begin{align}
\label{eqfssol}
& \left( \begin{matrix}
\hat{w}\\ 
\hat{b}
\end{matrix}\right ) = 
\left ( \hat{\Phi}^{\intercal}_e \hat{\Phi}_e + \frac{\mathit{I}_{m+1}}{\gamma} \right )^{-1} \hat{\Phi}^{\intercal}_e y,
\\ \nonumber \\
\text{where} \hspace{10pt}
& \hat{\Phi}_e = \begin{pmatrix}
\hat{\phi}_{1}(x_1) & \cdots & \hat{\phi}_{m}(x_1) & 1\\ 
\vdots &  \ddots & \vdots & \vdots\\ 
\hat{\phi}_{1}(x_n) & \cdots & \hat{\phi}_{m}(x_n) & 1
\end{pmatrix}. \nonumber
\end{align}

In the above formulation, $\hat{\phi}(x_k)$ is an approximation to the true feature map $\phi(x_k)$ which is related to the kernel $K(x_i, x_j) = \phi(x_i)^{\intercal} \phi(x_j)$ (Mercer's theorem). The approximate feature map $\hat{\phi}(x_k)$ is calculated using the Nystr\"om method as outlined in \cite{DeBrabanter2010,Mall2015} and \cite{Mall2013}. A low rank approximation to the kernel matrix is constructed by iteratively calculating a subset of the original data which maximizes the \textit{Quadratic R\`enyi Entropy}. This procedure of extracting $\hat{\phi}(x_k)$ from a data set, given a kernel function, is called \textit{Automatic Feature Extraction} (AFE).

Kernel based models are sensitive to hyper-parameters. In the case of FS-LSSVM we have to tune the model with respect to $\gamma$ the regularization parameter and the parameters of the kernel chosen. Models are generally compared with their cross-validation performance in which case the objective cost function with respect to the hyper-parameters is in general non-smooth and non-convex. Gradient free methods like Grid Search, Nelder Mead \cite{Nelder1965} and Coupled Simulated Annealing \cite{Xavier-De-Souza2010} are suitable to tackle the problem of model selection for FS-LSSVM based kernel models. Algorithm \ref{lssvmalgo} explains the steps involved in tuning the FS-LSSVM model with the bold part representing our contributions in this paper, which have been implemented in a MapReduce setting.

\begin{algorithm}[!htbp] \label{lssvmalgo}
\SetAlgoLined
\KwData{Data Set, Kernel, Global Optimization routine, grid parameters}
\KwResult{Proposed Tuned FS-LSSVM model}
 Pre-process the data by mean scaling.\;
 \textbf{Calculate the prototype set by maximizing the Quadratic R\`enyi Entropy in parallel using MapReduce.}\;
 Initialize a grid for the hyper-parameters\;
 \While{termination of global optimization routine}{
  Initialize the kernel using the hyper-parameters.
  Do AFE on the kernel matrix constructed from the prototypes, using the Nystrom method\;
  evaluate the cross validation score for the particular hyper-parameter values\;
 }
 \caption{Tuning FS-LSSVM}
\end{algorithm}
