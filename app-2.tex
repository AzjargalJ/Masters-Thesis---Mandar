\chapter{Statistical Learning/Machine Learning reference}
\label{app:2}
\section{Bias-Variance Trade off}
\label{app:biasvar}
In order to choose better performing models for prediction or classification, one must construct an expression of their error given a data set from which they are built. Suppose that we are approximating a function $y_i = f(x_i) + \epsilon$ using data $x_i$ drawn from a domain $X$ given by a distribution $F(x)$, $\epsilon$ is assumed to be noise with zero mean and variance $\sigma^2$. An estimate $\hat{f}(x)$ is constructed via the learning process. The bias variance trade off refers to the implication of decomposing the error of predictive models into three components \ref{eq:bias}.

\begin{itemize}
\item Bias: The error of the model with respect to the function that is being approximated.
\item Variance: Sensitivity of the model with respect to the training data provided to it.
\item Irreducible error $\sigma^2$
\end{itemize}

\begin{align} \label{eq:bias}
& \mathrm{E}\Big[\big(y - \hat{f}(x)\big)^2\Big] = \mathrm{Bias}\big[\hat{f}(x)\big]^2 + \mathrm{Var}\big[\hat{f}(x)\big] + \sigma^2 \\
& \mathrm{Bias}\big[\hat{f}(x)\big] = \mathrm{E}\big[\hat{f}(x)\big] - f(x) \\
& \mathrm{Var}\big[\hat{f}(x)\big] = \mathrm{E}\Big[ \big( \hat{f}(x) - \mathrm{E}[\hat{f}(x)] \big)^2 \Big]
\end{align}

This leads to the observation that when we decrease the bias of an estimator $\hat{f}(x)$ (ex. Use a polynomial estimator instead of a linear one), we increase the sensitivity (or \emph{variance}) of the estimator to unseen data. While it may predict with a high accuracy points in the training it has a tendency to \emph{over fit} the training set and hence have poor out of sample accuracy. This is precisely the \emph{bias-variance} trade off.

\section{AFE: Nystr\"om Method} \label{app:nystrom}

Let $X_k \ \in \ \mathbb{R}^d \ , \ k = 1, \hdots ,n$ be a random sample drawn from a distribution $F(x)$. Let $C \in \mathbb{R}^d$ be a compact set such that, $\mathcal{H} = \mathcal{L}^2(C)$ be a Hilbert space of functions given by the inner product \ref{eq:rhksi}. Further let $M(\mathcal{H}, \mathcal{H})$ be a class of linear operators from $\mathcal{H}$ to $\mathcal{H}$.  

\begin{equation} \label{eq:rhksi}
<f,g>_{\mathcal{H}} = \int f(x)g(x) dF(x) 
\end{equation}

Automatic Feature Extraction (AFE) using the Nystr\"om method \cite{nystrom} aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels \cite{Mercer}, as shown below \ref{eq:mercer}. It is well known that Mercer kernels form a \emph{RHKS} of functions. Every Mercer kernel defines a unique RHKS of functions as shown by the Moore-Aronszajn theorem \cite{N.Aronszjan1950}. For a more involved treatment of \emph{RHKS} and their applications the reader may refer to the book written by Bertinet et.al \cite{RHKSbook}.

\begin{equation}\label{eq:mercer}
K(x,t) = \sum_i{\lambda_i \phi(x)\phi(t)} 
\end{equation}

Mercer's theorem \cite{Mercer} states that the spectral decomposition of integral operator of $K$, $\mathcal{T} \in M(\mathcal{H},\mathcal{H})$ defined as \ref{eq:Fred} yields the eigenfunctions which span the RHKS generated by $K$ and having an inner product defined as \ref{eq:rhksi}. 

\begin{equation} \label{eq:Fred}
(\mathcal{T}\phi_i)(t) = \int K(x,t) \phi(x) dF(x)
\end{equation}

Equation \ref{eq:Fred} above is more commonly also known as the Fredholm integral equation of the first kind. Nystr\"om's method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set $X_k \ k = 1, \hdots, m$ and calculating its spectral decomposition consisting of eigenvalues $\lambda_k$ and eigen-vectors $u_k$. This yields an expression for the approximate non-linear feature map $\hat{\phi} : \mathbb{R}^d \longrightarrow \mathbb{R}^m$.

\begin{equation}
\hat{\phi}_{i}(t) = \frac{\sqrt{m}}{\lambda_i}\sum_{k=1}^{m}K(X_k, t)u_{k,i}
\end{equation}
