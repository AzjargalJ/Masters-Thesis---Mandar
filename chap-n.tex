\npdecimalsign{.}
\nprounddigits{3}
\chapter{Experiments}
\label{cha:n}
The following hardware mentioned in table \ref{table:hardware} is employed to execute \textit{FS-Scala} on a set of benchmark data sets. Machine number 1 is at the Department of Electrical Engineering, KU Leuven. The distributed FS-LSSVM implementation in \emph{FS-Scala} inside a local \emph{Apache Spark} application running concurrently on each core of the host machine.

\begin{table*}[!htbp]
\caption{Hardware Specifications}
\label{table:hardware}
\adjustbox{max width=\textwidth}{
\begin{tabular}{c c c c}
 \textbf{Machine No.} & \textbf{Machine} & \textbf{Configuration} & \textbf{Data Sets} \\
 \hline
 1 & sista-nc-3 & 40 core 64GB RAM & \textit{Adult} and \textit{Forest Cover Type}\\
 2 & Laptop PC & 4 core 8GB RAM & \textit{Magic Gamma}
\end{tabular}
}
\end{table*}

\section{Data Sets}

Experiments are carried out on the \textit{Magic Gamma} Telescope, \textit{Adult} and \textit{Forest Cover Type} data sets available from the UCI Machine Learning Repository \cite{Lichman:2013}. Below we give a short description of each.

\begin{itemize}
\item Magic Gamma: The data is generated by the registration of high speed gamma particles measured by a ground based atmospheric Cherenkov gamma telescope. Each entry consists of 10 numerical attributes and a binary class attribute. 

\begin{table*}[!htbp]
\caption{Magic Gamma Metadata}
\label{table:magicgamma}
\centering
\adjustbox{max width=\textwidth}{
\begin{tabular}{ l l}
%\hline
\textbf{Name} & \textbf{Value}\\
\hline
Training samples & 18792 \\
Test Samples & 228 \\
Features & 10\\
%\hline
\end{tabular}
}
\end{table*}


\item Adult: This is based on a a census study carried out in 1994, the data consists of 6 numerical attributes and 8 categorical attributes. The target attribute is binary class value, which indicates if the given individual has an annual income more than $50 000$\$.

\begin{table*}[!htbp]
\caption{Adult Metadata}
\label{table:adult}
\centering
\adjustbox{max width=\textwidth}{
\begin{tabular}{ l l}
%\hline
\textbf{Name} & \textbf{Value}\\
\hline
Training samples & 29310 \\
Test Samples & 3251 \\
Features & 13\\
%\hline
\end{tabular}
}
\end{table*}


\item Forest Cover Type: This consists of cartographic data collected by the US Forest Service (USFS) on $30 \times 30$ metre cells which have can have seven different forest cover types. For the purposes of the experiments below, a binary classification problem is constructed which consists of recognizing cover type class two from the other cover types. 

\begin{table*}[!htbp]
\caption{Cover Type Metadata}
\label{table:forestcover}
\centering
\adjustbox{max width=\textwidth}{
\begin{tabular}{ l l}
%\hline
\textbf{Name} & \textbf{Value}\\
\hline
Training samples & 523076 \\
Test Samples & 57936 \\
Features & 53\\
%\hline
\end{tabular}
}
\end{table*}

\end{itemize}

\section{Methodology}
The performance of various kernel based FS-LSSVM models is carried out for various values of the experimental parameters listed in table \ref{table:param}. Performance metrics given in table \ref{table:metrics} are computed for each cross validation and used to choose the best performing model during model tuning. To calculate the final performance estimate of each model given the number of prototypes, kernel and grid parameters, each experiment is repeated four times, the mean and standard deviation (in brackets), of the classification accuracy, area under ROC curve and execution time are recorded and presented in tables \ref{table1}, \ref{table2} and \ref{table3} for the \textit{Magic Gamma}, \textit{Adult} and \textit{Forest Cover Type} data sets respectively.

\begin{table*}[!htbp]
\caption{Experiment Parameters}
\label{table:param}
\adjustbox{max width=\textwidth}{
\begin{tabular}{ c l r}
%\hline
\textbf{Name} & \textbf{Meaning} & \textbf{Values} \\
\hline
Kernel & Kernel Type & Linear, RBF, Polynomial, Laplacian \\ 
Prototypes & No. of prototypes & 50, 100, 200\\ 
Global Opt. & Global optimization & gs: Grid Search, csa: Coupled Simulated Annealing\\
Grid Size & Number of grid points  & 2,3,4\\
$nTrials$ & Number of times each experiment is carried out & 4\\
$CGIterations$ & Number of local CG iterations & 35\\
$nFolds$ & Number of folds: fast CV \ref{cvmr}\\
$CSAIterations$ & Number of CSA iterations when used as Global Opt. & 5\\
%\hline
\end{tabular}
}
\end{table*}

\clearpage

\begin{table*}[!htbp]
\caption{Metrics}
\label{table:metrics}
\centering
\adjustbox{max width=\textwidth}{
\begin{tabular}{ c l}
%\hline
\textbf{Name} & \textbf{Meaning}\\
\hline
Accuracy & Classification accuracy on test set averaged over \\
ROC area & avg. area under the ROC curve \\
Execution Time & avg. execution time of FS-LSSVM model tuning in seconds\\
%\hline
\end{tabular}
}
\end{table*}

\section{Results}

\subsection*{Magic Gamma}
The performance of binary FS-LSSVM classifiers on the \textit{MAGIC Gamma} Telescope Data Set obtained from the UCI Machine Learning Repository, are summarized in Table \ref{table1}. FS-LSSVM models trained with polynomial kernels give better classification performance than the RBF and Linear counterparts, on the \textit{MAGIC Gamma} data. Judging from the difference in the performance results between polynomial and linear LSSVM models, it can be said that the \textit{Magic Gamma} data set has weak non linear behavior.
 
\DTLloaddb{magicgamma}{resultsMagicGammaProc.csv}
\begin{table*}[!htbp]
\sisetup{round-mode=places}
\caption{Magic Gamma Test Results}
\begin{center}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l l l l l l l}\label{table1}
\bfseries Kernel & \bfseries Prototypes & \bfseries Global Optimization & \bfseries Grid Size 
& \bfseries Accuracy & \bfseries ROC area & \bfseries Execution Time%
\DTLforeach{magicgamma}{\kernel=kernel,\prototypes=prototypes,
\globaloptimization=globaloptimization,\gridsize=gridsize,
\acc=accuracy,\stdA=stdA,\maxF=maxF1score,
\stdF=stdF,\areaunderROC=areaunderROC,\stdR=stdR,\time=time,\stdT=stdT}%
{%
  \\\kernel & \prototypes & \globaloptimization & \gridsize & 
  $\acc(\stdA)$ & $\areaunderROC(\stdR)$ & $\time(\stdT)$
}%
\end{tabular}
}
\end{center}
\end{table*}

\subsection*{Adult}
The performance of binary FS-LSSVM classifiers on the \textit{Adult} Data Set, are summarized in Table \ref{table2}. FS-LSSVM models trained with exponential kernels give better classification performance than the RBF and Linear counterparts, on the \textit{Adult} data. For both the data sets one sees a pattern emerging that tuning kernel models with CSA gives better results than naive \textit{Grid Search} based hyper-parameter optimization.

\DTLloaddb{adultres}{adultres.csv}
\begin{table*}[!htbp]
\caption{Adult Data Set Test Results}
\begin{center}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l l l l l l l}\label{table2}
\bfseries Kernel & \bfseries Prototypes & \bfseries Global Optimization & \bfseries Grid Size
& \bfseries Accuracy & \bfseries ROC area & \bfseries Execution Time%
\DTLforeach{adultres}{\kernel=kernel,\prototypes=prototypes,
\globaloptimization=globaloptimization,\gridsize=gridsize,
\gridresolution=gridresolution,\acc=accuracy,\stdA=stdA,\maxF=maxF1score,
\stdF=stdF,\areaunderROC=areaunderROC,\stdR=stdR,\time=time,\stdT=stdT}%
{%
  \\\kernel & \prototypes & \globaloptimization & \gridsize & 
  $\acc(\stdA)$ & $\areaunderROC(\stdR)$ & $\time(\stdT)$
}%
\end{tabular}
}
\end{center}
\end{table*}


\subsection*{Forest Cover}
Due to the computational overhead, only CSA based model tuning is employed in the experiments on the \textit{Forest Cover Type} data, yet some clear trends can be observed in table \ref{table3}. It is observed that the Laplacian kernel based models outperform the RBF as well as the linear LSSVMs. This is because the Laplacian kernel relying on the $L_1$ norm is more robust to outliers and leads to a matrix ($A$) with a better condition number, leading CG iterations that converge in fewer iterations.
 
\DTLloaddb{forestcoverproc}{forestcoverproc.csv}
\begin{table*}[!htbp]
\caption{Forest Cover Type Data Set Test Results}
\begin{center}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l l l l l l l}\label{table3}
\bfseries Kernel & \bfseries Prototypes & \bfseries Global Optimization & \bfseries Grid Size 
& \bfseries Accuracy & \bfseries ROC area & \bfseries Execution Time%
\DTLforeach{forestcoverproc}{\kernel=kernel,\prototypes=prototypes,
\globaloptimization=globaloptimization,\gridsize=gridsize,
\gridresolution=gridresolution,\acc=accuracy,\stdA=stdA,\maxF=maxF1score,
\stdF=stdF,\areaunderROC=areaunderROC,\stdR=stdR,\time=time,\stdT=stdT}%
{%
  \\\kernel & \prototypes & \globaloptimization & \gridsize & 
  $\acc(\stdA)$ & $\areaunderROC(\stdR)$ & $\time(\stdT)$
}%
\end{tabular}
}
\end{center}
\end{table*}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
